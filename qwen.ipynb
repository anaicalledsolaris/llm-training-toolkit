# Jupyter Notebook content from QwenLM/Qwen/recipes/quickstart/qwen.ipynb

## Qwen Quick Start Notebook

This notebook shows how to train and infer the Qwen-7B-Chat model on a single GPU. Similarly, Qwen-1.8B-Chat, Qwen-14B-Chat can also be leveraged for the following steps. We only need to modify the corresponding `model name` and hyper-parameters. The training and inference of Qwen-72B-Chat requires higher GPU requirements and larger disk space.

## Requirements
- Python 3.8 and above
- Pytorch 1.12 and above, 2.0 and above are recommended
- CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)
We test the training of the model on an A10 GPU (24GB).

## Extra
If you need to speed up, you can install  `flash-attention`. The details of the installation can be found [here](https://github.com/Dao-AILab/flash-attention).

### Step 0: Install Package Requirements